{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Transformers On Large-Scale Graphs with Bayan Bruss - #641", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " Thanks to our friends at Capital One for their continued support of the show and their sponsorship of this episode. Capital One's AI and machine learning capabilities are central to how it builds products and services, and they're now at the forefront of what's possible in banking. Whether helping consumers shop more safely online, giving customers new insights into their finances via award-winning mobile apps, or advancing research into cutting-edge applications of AI and machine learning, Capital One is using technology to make banking better. To learn more about Capital One's machine learning and AI efforts and research, visit twiimlai.com slash go slash Capital One. Alright everyone, welcome to another episode of the Twiiml AI Podcast. I am your host, Sam Sherrington. Today I'm joined by Bayan Bruce. Bayan is Vice President of Applied Machine Learning Research at Capital One. Before we get into today's conversation, be sure to take a moment to head over to Apple Podcasts or your listening platform of choice. And if you enjoy the show, please leave us a five-star rating and review. Bayan, welcome back to the podcast. Thanks, Sam. It's great to be here again. It's great to have you back on the show, and congratulations are in order. The last time you were here, you were Senior Director of Applied ML Research. That's right. It's very exciting and it's a sign of the continued growth and investment that Capital One has made in this area. So it's exciting for me personally and then also for the team. Awesome. Since you are a repeat guest, we'll keep introductions brief, but I would love for you to share briefly a little bit about your background. And then we will jump right into our conversation, which is going to focus on some of the research that you and your team are presenting at ICML Conference. Great. Yeah, brief introduction for those of us who didn't hear the last podcast. So I lead our Applied Machine Learning and AI as the new addition to that title team at Capital One. And our main focus is how do we take all of the amazing academic and industry research and shorten the time until it can be used in production systems across the company, across our applications. I think, as I mentioned last time, Capital One has a very long history of using machine learning in many of our most important applications. And as part of that history and the foundation that we built, we know that we have to stay on top of the advancements in the technology to be competitive and to provide the best possible experiences for our customers. And so we use our applied research function as a way to shorten the time from when there's been some great breakthrough outside the company to when we can use it for the various things that we do. And your team has a couple of papers at the conference we'll be talking about. The first that we'll dig into is called Identifying Interpretable Subspaces in Image Representations. Give us a high level overview of that paper and what it's hoping to accomplish. Sure. I guess let me back up and provide a little bit of additional context. One of the things that we do as a team in order to, as I say, accelerate that adoption of research and technology is we partner very closely with a wide range of academic partners across a number of universities. University of Maryland, Carnegie Mellon, MIT, Columbia, NYU, and others. And we work side by side with researchers, grad students, and we work on publications with them so that we can understand where the state of the art lies and co-generate that knowledge and then take that back and quickly apply it to the things that we're seeing inside the company. So in this particular case, this was a paper that we worked on with some colleagues at the University of Maryland, Nihal Khalibat, Shweta Bharadwaj, Hamid Firouz, Maziar Sanjavi, and Sohail Faizi. And as I mentioned, stepping back again at Capital One, and I actually think this is more broadly across industry, while the big focus in the headlines and other places is generative AI, the workhorses of industrial machine learning applications are still discriminative. There are models that are taking in inputs, making classification decisions, making regression decisions. They are taking those decisions and then applying business logic to them and then determining what happens in a given application or a given system that we use. And given that we have so many of these models that are discriminative in nature and they're running in all of these different applications that we have, one of the best and easiest ways for us to take deep learning and AI technology and quickly integrate it into the various systems that we have is through embeddings, is through representation learning. And you actually see this in a number of industries, this isn't just Capital One. Representation learning and high quality embeddings has become the workhorse of many, many applications of machine learning today. And the way that it typically works is you have some complex, high dimensional data set, you have some pre-trained embedding generation model that produces embeddings to some centralized repository. And then downstream applications can come to those embedding repository and do a wide range of tasks on top of that. Either they can take those embeddings and they can do similarity search, nearest neighbor functions on top of the embeddings. This is why we're seeing such a huge proliferation of things like vector databases or another common path, especially when you have a lot of machine learning models, is that the downstream models can use those embeddings as inputs in addition to other features that they might be using to make predictions. And they can thereby take advantage of all the representation learning power of a deep learning model without having to be fully integrated with that deep learning system. And so we actually see oftentimes like a mix and match where you'll have a traditional tree based model using embeddings as an input. And it's kind of a way to accelerate deep learning into our various systems. But it leaves this gap, which is that traditional features and traditional models were hand engineered by the data scientists who built the models. They had an intuitive sense of what those features were when they put those features in the models and deploy the models, when they do feature importance or any kind of explanation technique, post hoc explanation technique, like the shop values. What they are learning is this feature was important to this prediction. And because I know that this feature was and where it came from, I understand and kind of implicitly what was going on that led that prediction to happen or why the model made that prediction. When you take those traditionally hand engineered features where there was some clear sense of what they mean, then you append a 64 dimensional or 128 dimensional feature vector, 256 dimensions embedding and pass that into the model. And then you ask the same question of it. It's they might say, oh, well, these two features that you understand were important. And then embedding dimension six and embedding dimension 53 were important. And as a data scientist or as somebody who's in the business trying to understand model behavior, that becomes very challenging to interpret. And so having a agnostic way of understanding representations and what goes into those representations is a big area of interest to us. And so that is the setup for identifying interpretable subspaces in image representations. That's a great setup in context for the paper. One of the immediate things that jumped out at me just kind of looking through it is that it throws around the term feature, but everything's a feature in these systems to some degree or another. In this context, you're talking about dimension 53 in some long embedding as opposed to full embedding or something else. You've got these big embeddings. Are they individual dimensions or are they pairs or tuples of dimensions that kind of carry the meaning here? That's a great question. So we started looking at individual dimensions and asking exactly as you say, like for pick a single element in the given embedding. What can we learn about what that embedding captures or what that dimension captures amongst all the dimensions? And we can talk about how we go about doing that. One of the things that we observed is that the kind of interpretability, you can almost call it like the purity of a given dimension is actually fairly low and only a small percentage of all the dimensions on average provide unto themselves an interpretable kind of set of things that you might look at. And actually what ends up happening is if you combine multiple dimensions and use that combination, you get these more interpretable substasis in a much larger way. And I think it makes sense because of the way that neural architectures work. Oftentimes, that dimension will encode multiple different pieces of information. And it is only in combination with other dimensions that encode other pieces of information that the combination of two actually start to become interpretable. The model that you use for the embeddings here is SIMCLR. It's contrastive. It's a contrastively learned embedding model. You also use the concept of contrastive concept extraction. Are those two, is the use of contrastive techniques kind of coupled between those two or are they kind of independent decisions? They're independent decisions. Contrastive learning obviously being a fairly powerful paradigm, both in how you generate representations, but also when we think about interpretability and explanations, contrasts are a common way that humans understand the world. It's not just this and this. It's this and this and not that is an easy way to go about differentiating between things. And so I guess I'll walk through exactly what is going on so we can describe what these contrastive images are. I recognize that I've jumped into the weeds here. So definitely do that. That's fine. Yeah. So at a high level, the way this technique works is that we're focusing for now on vision models, but this will potentially be informative for a wide range of other modalities besides vision, for instance, graph structured data and others. But you start with a pre-trained representation learning model for vision, for images, and you take the images that are in that data set and for each dimension in the representations that are generated for those images, you identify the most highly activating images for that dimension. So essentially use a traditional, in this case, we use GradCam, but you can use other methodologies for local explanations to understand images where that dimension has a much higher capture of the activations of that image. And then even within those images, we want to then get down to a feature crop. So we don't want to look at the whole image because different things in the image might activate different dimensions in the embedding space. And so we take the images that are most activated for that dimension, and then we look within that image and find actually which pixels are most activated, and then we crop around those. And so now you have for that dimension a set of cropped images. So in the example we have a bunch of pictures of like a shaggy dog, but it's not the whole picture. It's like, you know, little subspaces like the nose and the eyes instead of the whole picture. And then you take another model, and this is where the benefit of pre-training becomes pretty powerful. So we have these language image models, these captioning models like Clip, and you can take the feature crop and you can pass it into a language image model, and it can generate some text, some descriptive language about what it's seeing in that feature crop. So that essentially becomes the natural language descriptor of the subset of the image that corresponded to the representation dimension that you started with. And so that's how you map from representation dimension down to a natural language interface so you can understand what's going on in that dimension. Now one of the things we found is that just doing this effort to go from highly activating to natural language alone did not provide super interpretable kind of explanations of that dimension. And that's where contrastive explanations come in. So instead of just relying on the things for that dimension that are highly activating, we also use images that are lowly activated for that dimension. So these are the things that that dimension don't encode or that aren't important to that dimension. And we go through that same process and essentially what you can then do is subtract from the descriptions of the highly activated images, the descriptions of the lowly activated images, and get to a more refined understanding of what is going on in that dimension, essentially saying, let's remove anything from our description of this image or of this dimension that would generally just appear in the data set or that would randomly appear in the data set. And so that's where contrastive explanations come from. And it really boosts the quality and the interpretability of the descriptions that we're looking at. How is that subtraction happening? Is that how you giving both to a LLM and asking it to differentiate the concepts? Or is it something simpler, removing keywords or something? It's something simpler. It's removing keywords. So when you think about hundreds or thousands of cropped images that you've attributed to a given dimension. It's a lot of inferences on an LLM. It's a lot of inferences on an LLM. And it's also like, it essentially becomes a book, right? You add a thousand sentences and just to understand that one dimension, you have to read through all of them. And so we took a bit of a bag of words approach. So removing stop words, removing, essentially getting down to the highest frequency words that the LLM was generating. And so then the same thing as you remove from that list of highest frequency words that the LLM is generating for the activated images, you subtract from that the highest frequency words for the lowly activating ones. And so you essentially get this removal of descriptors or descriptive tags that are more common in the overall data set or more common in the things that don't, that shouldn't be in that representation space. This process of identifying the highly activating images, is that starting from an individual dimension and working backwards or is it a batch task that happens on the training data set that identifies what dimensions an individual image activates? You can still use the training data set or you can use any other data set after the model has been trained. It doesn't have to be the same data set that was used in the pre-training. But the idea is you compute the gradients with respect to that one dimension. And as you compute the gradients with respect to that one dimension, you'll see the highest gradients correspond to a subset of images. And those are the images that you select. You're essentially working your way back through the model that created the representations to get there. Talk a little bit about the prior work in the space and, you know, against which you benchmarked your results. Yeah, I think the couple of things that were relatively new, actually I should say are very new, the contrastive concept extraction is novel in this space. And some of the other methodologies often will do a much simpler approach where they just take images that correspond to given dimensions and try to assign some kind of language tag to them. The process of doing the contrastive subtraction is, I think, the thing that was most beneficial here. And I think more generally, there's a broad set of research on explainability that we've been looking at that goes beyond just individual dimensions. And I think this speaks to what this research really is beneficial for us when I step back and think about it. You know, a lot of explainability research, there's two tiers of explainability types. There's either, how do I explain a given prediction that a model's made? How do I explain why we classified this transaction as fraudulent or not fraudulent? And that'll be essentially a vector over the inputs to that model and weights that correspond to which of the inputs matter the most. And then there's global feature importance, which is this other paradigm, which is on average, what are the things that this model uses to make decisions? And that leaves this big gap in how models behave, where we look at different slices, either slices horizontally at, what is the model doing for certain subpopulations, or in this case slices vertically or different dimensions in the input. And I think we've been using representation learning in a few cases, but this idea that you can then map backwards from a given dimension actually plays very well with how we think about their usage in these downstream models, because we're often asking these local questions around for a given prediction, what mattered the most. And so therefore having the ability to say, not just these dimensions are important, but then to go backwards and say, okay, from the inputs, these are the subset of inputs that correspond to that dimension. And so then we can inspect those inputs for a given model. One thing I will add is the usage these different subspaces for error correction in model diagnosis. So by having a more intermediate picture of what's going on in model behavior, in this case, looking at individualized dimensions or subspaces across different dimensions, you can then draw correspondence between model errors in the downstream model and those given subspaces that are important. Or if there's a, I guess, let me explain it this way. So say you have a group of images that your model classifies incorrectly, you can then see, okay, which subspaces do those images belong to, almost treating it as the inverse problem. So take those feature crops and say, okay, which of these existing groups that I've already identified, does this belong to which dimensions does this correspond to, and start to identify model behavior that you could then correct. You could say, well, it's actually attending to this cropped feature within this one. And it's actually learning from the wrong information about this image. And there's this whole notion of spurious features, spurious correlations in a lot of deep learning where models will, especially computer vision models, will learn from anything you give them, whether you intend for them to learn from it or not. And if in the training set, the things that are most predictive of the thing you're trying to predict are not actually indicative of that outcome, the model will still learn it. So the classic example is the husky on the white background is classified as a wolf because the model is picking up on snow, not on anything about the dog. In our case, we show some examples where the models classified a flock of geese as an airplane wing because of the blue sky. Using this methodology, you can not only see, okay, that the model got the prediction wrong, but you can actually see what the model thought was in the picture. Right? So you can see the model thought that this was a airplane wing or something like that. And then you can visually inspect it and say, okay, that's actually a goose. And so then you can go back and tweak your training data set, or you can tweak your learning procedure to make sure that your model's not picking up on those kinds of spurious correlations. So, you know, from a model diagnostics perspective, becomes a bit of a debug tool. Exactly. Which is very hard in some of these deep learning methodologies. Awesome. And then was there a particular benchmark that you compared performance against? There are a few benchmarks pull up the exact reference to the benchmark. One of the comparisons is against a Milan. Yes. Is that another approach, Hernandez and all? That is another one. And then one of the other interesting things that our partners at Maryland did in this particular case is we also did a human study, which I think is actually quite important in explainability research. And so what they did is they actually had individuals look and rate the different explanations and the different descriptions of the dimensions and say how accurate they were just by what they're seeing in the images, as well as kind of how useful they are in understanding what's going on with the model. Awesome. The next paper that we wanted to talk about is GOAT, a global transformer on large scale graphs. It is a planting a stake in the ground, making a statement with the name of that paper. Yeah, it's a great name. I guess the alternative would have been GLOTE, which may be even more of a statement. The many years that I've kind of followed some of the work at Capital One, graphs and graph neural networks has always been kind of a common theme and something that the teams are very interested in, presumably related to their utility and fraud use cases and others. And just based on the title here, you're applying the hot new thing and transformers to the thing that you've been concerned about for a very long time. So with that, give us the high level of a view of this one. Yeah, that's exactly right. Financial services, by and large, operates on top of very large natural graphs or networks. In our case, you can imagine every time that you swipe a credit card, you are establishing an edge between you and the merchant. That scales up to functionally the many tens of millions of a node network with billions of edges. And it contains a lot of very useful structural information that we can use in a wide range of applications. So we've been looking at, as you say, we've been looking at how do we apply graph neural networks and graph representation learning for a number of years. This is not a new area for us, but with the dominance of transformers in recent years, it makes sense to look at them in the space of graphs. And one of the big challenges is that transformer architecture's particularly self-attention is on long sequences in the language space is something that has a polynomial time complexity. It's N squared. So you're essentially taking this N by N matrix of all the elements in a given sequence and you have to compute that. That's a very computationally expensive task. And then on a graph, the corresponding notion of a sequence on a graph is a neighborhood. So we think about a neighborhood, what we have is all the... If you're a single node in the graph, your neighborhood is all the nodes that you're connected to. That is the size of the N when you are computing attention. If you go two hops, which means that you go to your neighbor's neighbors on the graph, then the N is now not just your neighbors, but all of their neighbors. So now that's the square of all of your neighbors, neighbors and your neighbors. And as you go outwards to three hops, four hops, however large your graph is, so now you not only have a square in the size of the number of elements that you have to compute attention over, that is itself growing exponentially, because graphs obey these power law in the relationship between neighborhoods over the scale of the graph. And so it becomes computationally intractable to actually do attention on anything larger than very small networks. Now, it has been shown that attention is quite powerful in the graph domain. And so there have been a number of papers over the years that have looked at how do we apply this notion of a graph attention. And what has primarily happened is that the focus of those efforts has been on very small graphs. I think a lot of things like molecules or other very small synthetic graphs where you're doing maybe not large scale node classification, you're doing like a graph classification. You're asking, what is this graph that I'm looking at? And in those domains on those small graphs is actually quite powerful, but to scale it up to the size of a graph that we have is computationally intractable. And so this research set out to propose some methodologies to get around that computational barrier so that we can do attention on a larger scale. The other benefit of doing attention on a larger scale, besides just being able to do attention on a larger scale, is that most graph neural networks are built on the notion of homophily. Homophily being the principle that you are more like the things that you are connected to, or a node is more like its neighbors than it is nodes that it is further away from. And that is true of many of many networks, particularly in the social sciences. However, there are also a lot of networks where that is not true. And there is a notion of heterophily, meaning that you are dissimilar from your immediate neighborhood. And then I think the reality is that a lot of tasks are some mix of the two, where in certain situations a node you might want to have information about the node's neighborhood because it's highly predictive. And in other situations, you're actually looking for relationships between that node and nodes that are far away from it on the natural graph. And so having methodologies that don't require you to a priori pick, I'm going to use this because I think it's a homophily principle, or I'm going to use this because I think it's a heterophily principle, but can then just work as a whole, becomes quite beneficial from a practitioner's perspective because you don't always know, and you don't always want to tune that. It's not a matter of just picking a parameter in a model. What you're picking is an entirely different model class to have to switch back and forth. And that can be quite challenging. And so this paper sets out to solve both of those things. Can we scale to very large graphs and still apply attention across the entire graph instead of just a local neighborhood? But then can we also retain certain localized information? So the way that the research takes this problem and makes it tractable is through this notion of a codebook, which is essentially taking the entire graph and doing a k-means aggregation into smaller clusters of the graph and assigning every single node in the graph into one of these clusters. And then rather than having to do your global attention over every node in the entire graph when you're making a prediction, now you only have to do your attention over the space of clusters that you have defined, which you can fix in dimensionality. It becomes much more tractable if you want to make it really, really efficient. You could say there's only three rows in this codebook, and I just want you to compute the global attention on those three. Or if I want to expand and look at a much finer grained perspective, you can go all the way out. And so essentially that becomes a hyperparameter that you can tune to understand how much global information you want to collapse into this lower dimensional space or how much you want to leave in there. And that allows you to attend to every node in the graph. Of course, it's an approximation of every node in the graph, but you can attend to every approximate node in the graph through this codebook. And then there's an additional self-attention mechanism, which allows you to attend similarly to at least a couple hop neighborhood around the given node. And so then that gives the node classification task the ability both to go global in this approximate way, as well as to go local in a more traditional way and see significant performances. And now what's really interesting about the results is that they show that this model is capable of performing very, very well, not just on homophilic graphs and not just on heterophilic graphs, but on both. And so it balances performance. It can learn from the structural properties of the graph and from the task at hand which pieces of information it needs to attend to in order to perform optimally at the node classification. And so a lot of that structure is not baked into the model. The model gets to learn it directly from the data, which is a really neat approach. You know, when I think about that n squared problem and the nature of these graph connectivity matrices, one of the things that jumps out at me that you might want to exploit is sparsity. What are the properties of these matrices that the codebook approaches exploiting? It is in a way it's exploiting sparsity in a way it's actually making the space denser. You think about projecting this graph into a lower dimensional space and then running your tension over that projection is essentially what is happening. That's going, you know, your original graph is the very sparse and you're going into something denser, but it's fixed in dimensionality and the dimensions that you want it to be fixed in. And that I think is what makes it more beneficial from an attention perspective. I tend to find sparsity to be a big problem for machine learning. I don't usually think of it as a benefit. In a lot of cases, models can't handle sparsity particularly well and we often try to use approximate techniques to remove the sparsity and to make things more dense. Even if you think about traditional embeddings, like going way back to Word2Vec, the goal there was to move away from one-hot encoded vectors where you could think of your document word matrix as this giant matrix of zeros and ones, much like a graph is a, you know, an adjacency matrix is a giant matrix of zeros and ones. And what researchers found very early on was that when you started to work with large vocabularies, it was impossible to model them effectively. And so you have to go from something very sparse to something very dense. And so I think the same property applies here. Yeah, that's interesting. I'm thinking of interviews I've done where folks are trying to exploit sparsity to accelerate things in various ways, but maybe the practical utility of those approaches is less so than moving away from the sparsity to another more dense representation. Yeah, I think certainly in certain model compression techniques and paradigms, you can exploit sparsity to like drastically reduce memory overheads and potentially computational efficiency. There are domains where that is true. From a learning algorithm perspective, I tend to think of sparsity as not being a friend of any algorithm that's trying to learn from the data. But there are probably a lot of areas below the learning algorithm itself where sparsity becomes quite beneficial. You mentioned that you can control the dimensions that you're reducing to in addition to this hyperparameter of the number of these clusters. Can you talk a little bit more about how you do that? It is specifically the clusters themselves. So you set that as a fixed dimension and the model learns to assign the nodes to those clusters. And essentially, the attention matrix is over the codebook itself, so over the cluster centroids. And so if you want to have 100 centroids, then you can set that as hyperparameter. If you want to have 10,000, you can set that as a hyperparameter. It fixes the dimensions of your attention matrix, essentially, is what you're allowing that to do. And so it's something you get to choose before you start modeling. But the only thing you're choosing is that one hyperparameter as opposed to some other semantic aspect of the dimensions. I thought I heard that in your explanation earlier. Yeah, in this case, that is true. It's deep learning, so there's a million hyperparameters and choices to make. But in this case, that is one of the main ones. Awesome. And how about for this paper evaluation, how did you evaluate the results? Yeah, so we compared against the state of the art in each of... I mentioned how does this model perform on homophilic graphs and how does it perform on heterophilic graphs? And so we compared to state of the art for each of those. Typically, most models either perform well on one or the other. And so GCN, JK, the graph attention network, LinkX and MixHop, these are four different models that we tested against, two for built on the premise of homophily and two that are built on the premise of heterophily, as well as some full-scale transformer architectures, which ran out of memory on every single test that we ran. And as I mentioned, these are all common benchmarks that we tested on, like the OpenGraph benchmark and SNAP and some other archive benchmarks. We perform, if not best on the tasks, second best across both heterophily and homophily and able to scale to very, very large size graphs. Got it. Can you talk briefly about where these couple of research directions take you next? I think on the first one on the interpretal images, I think the areas that I'm most interested in that research is how does it apply to other domains of deep learning? And so I guess bringing it back to the networks, if you, rather than having interpretable subspaces of an image, could you have an interpretable subspace of a network where you could say this particular dimension of an embedding corresponds to these nodes or these particular neighborhoods in the network that matter most for this embedding. One of the challenges there is that we lack the same image captioning models like CLIP that can generate text for a graph. We have these models that can generate text for language, but there aren't pre-trained generative models for other domains like networks. And so that might unto itself be an interesting line of research, which is can you align a network embedding to a language model so that you can generate descriptions of nodes in a graph? And that might be as simple as taking something like archive and predicting what the title of the paper is just by its network connectivity. But once you have that, then you can then use that to generate interpretable explanations of the representations. And then on the global transformer on large scale graphs, I think the big area of graph research that I'm very eager to see us continue to make progress on is in the temporal domain. How do we take these models, not just encode the network information, but also the dynamism of the graphs? Most industry graphs change over time, and they change in ways that are actually quite informative to a lot of tasks. And so that's where I'd love to see it head next. But there's also a lot of experimentation to be done with just these approaches on the stuff we already have before we have to try out for the next day of the art. Awesome. We'll of course be dropping links to these papers in the show notes page. Well, I didn't get to shout out the authors of the GO paper, so I'm going to give some love to my colleagues Keji, Kong, Jihoo, Chai, John Kirshenbauer, Ren Kun-Ni, and Tom Goldstein. Also, we are helping organize the workshop on machine learning and finance at KDD. Coming up, it's I think the seventh or eighth year in a row that that workshop is happening. Obviously, very excited again that it's back in person. It's always been a great workshop where if you're working on machine learning and finance, you get to learn and share with others. And on a similar vein, coming up later in the year, there is the ACM International Conference on AI and Finance. My colleague, Senthal Kumar, is on the steering committee for that and another wonderful way to get in touch with researchers who are working on AI and machine learning in the financial services sector. Awesome. Well, Bayan, it's great to have you back on the show. Thanks so much for taking the time to share what you're up to at ICML. Thank you. It was great talking to you, Sam. Thank you. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimlai.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "In this podcast episode, the host and guest discuss two research papers presented at the ICML Conference. The first paper focuses on identifying interpretable subspaces in image representations. The researchers propose a method to understand the features and dimensions of image embeddings, which can be challenging to interpret. They use a contrastive technique to generate descriptions of individual dimensions and combine multiple dimensions to create interpretable subspaces. The second paper introduces GOAT, a global transformer on large-scale graphs. The researchers address the computational challenge of applying attention mechanisms on large graphs by using a codebook approach. This allows attention to be applied to clusters of nodes rather than the entire graph, making it more tractable. The model performs well on both homophilic and heterophilic graphs, and the researchers aim to explore temporal dynamics in future research.", "podcast_guest": "Bayan Bruce", "podcast_highlights": "In this podcast episode, the host interviews Bayan Bruce, Vice President of Applied Machine Learning Research at Capital One. They discuss two research papers presented at the ICML Conference. \n\nThe first paper, titled \"Identifying Interpretable Subspaces in Image Representations,\" focuses on the challenge of interpreting high-dimensional feature vectors in deep learning models. They propose a method to identify interpretable dimensions in image representations by using a contrastive approach. The paper introduces the concept of contrastive concept extraction, which involves finding highly activating images for a specific dimension and extracting informative subregions from those images. The researchers also conducted a human study to evaluate the accuracy and usefulness of the generated descriptions.\n\nThe second paper, titled \"GOAT: A Global Transformer on Large-Scale Graphs,\" addresses the computational challenge of applying self-attention mechanisms to large graphs. The researchers propose a method that combines global and local attention to achieve scalability. They introduce a codebook approach where the graph is divided into clusters, and attention is computed only over these clusters instead of the entire graph. This allows the model to attend to every node in the graph in an approximate manner. The paper demonstrates the effectiveness of the proposed method on both homophilic and heterophilic graphs.\n\nThe main insights from the podcast include the importance of interpretability in deep learning models, the practical application of representation learning and embeddings in various industries, the need for scalable attention mechanisms in graph neural networks, and the challenges of modeling large-scale graphs.\n\nThe actionable recommendations discussed include exploring the use of interpretable subspaces in other domains of deep learning, such as networks, and investigating the incorporation of temporal dynamics in graph models. Additionally, the podcast highlights the upcoming workshop on machine learning and finance at KDD and the ACM International Conference on AI and Finance as opportunities for researchers in the financial services sector.\n\nThe significant arguments presented in the podcast revolve around the strengths and limitations of the proposed research papers. Some of the arguments relate to the trade-off between sparsity and density in machine learning, the challenges of interpreting complex high-dimensional representations in deep learning, and the need for scalable attention mechanisms in graph neural networks. The podcast also emphasizes the importance of evaluating research results against benchmark models and the potential impact of the proposed methods on various applications."}